{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    %cd \"/content/drive/MyDrive/Colab Notebooks/SIP_LSTM/\""
      ],
      "metadata": {
        "id": "62E1jvDimiqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsxhK7PpdB6D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Cleaned_scenari_validi.csv')\n",
        "\n",
        "dialogs = list(df['Replaced Signalling Description'].to_list())\n",
        "\n",
        "# Build SIP methods and return codes vocabularies\n",
        "methods = set()\n",
        "codes   = set()\n",
        "seqs = []\n",
        "max = 0\n",
        "for d in dialogs:\n",
        "    seq = []\n",
        "    for msg in d.split(':'):\n",
        "        tok = msg.split(',')[2]\n",
        "        if '-' in tok:\n",
        "            m, c = tok.split('-',1)\n",
        "            seq.append(m)\n",
        "            seq.append(c)\n",
        "            methods.add(m)\n",
        "            codes.add(c)\n",
        "        else:\n",
        "            seq.append(tok)\n",
        "            methods.add(tok)\n",
        "    if len(seq) > max:\n",
        "        max = len(seq)\n",
        "\n",
        "    seqs.append(seq)\n",
        "\n",
        "symbols = methods | codes\n",
        "symbols.add('<PAD>')\n",
        "\n",
        "message2idx = {m:i for i,m in enumerate(sorted(symbols))}\n",
        "\n",
        "print(seqs)\n",
        "print(message2idx)"
      ],
      "metadata": {
        "id": "htkWCk5vrsTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogs = []\n",
        "for s in seqs:\n",
        "    dialogs.append(''.join(s))"
      ],
      "metadata": {
        "id": "maxwA3lQ0i6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 1) Hyperparameters\n",
        "# -------------------\n",
        "M = len(message2idx) - 1 # Rappresenta il numero complessivo di tipi distinti di messaggi SIP (richieste e risposte) che possono comparire in un dialogo.\n",
        "LM = len(message2idx) # È la lunghezza del vettore one-hot usato per codificare ogni messaggio SIP.\n",
        "LN = max # È la lunghezza fissa delle sequenze “padded” in input alla rete. Ogni osservazione ​viene allungata aggiungendo zeri fino al valore LN.\n",
        "N = len(set(dialogs)) # Indica il numero di classi di output del modello, ovvero il numero totale di dialoghi SIP unici presenti nel training set. Ciascun dialogo è etichettato con un identificatore in {1, ..., N}\n",
        "units = 256 # 1043\n",
        "dropout_rate = 0.5\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "max_epochs = 200\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
        "\n",
        "print(\"M = \", M)\n",
        "print(\"LM = \", LM)\n",
        "print(\"LN = \", LN)\n",
        "print(\"N = \", N)"
      ],
      "metadata": {
        "id": "Rc2YzE57dI6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 2) Build LSTM models\n",
        "# -------------------\n",
        "def build_model_1():\n",
        "    m = Sequential([\n",
        "        LSTM(units, input_shape=(LN, LM)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(N, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def build_model_2():\n",
        "    m = Sequential([\n",
        "        LSTM(units, return_sequences=True, input_shape=(LN, LM)),\n",
        "        Dropout(dropout_rate),\n",
        "        LSTM(units),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(N, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return m"
      ],
      "metadata": {
        "id": "bOE1zqVidL8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 3) Load data and One-Hot Encoding\n",
        "# -------------------\n",
        "encoded_dialogs = []\n",
        "for s in seqs:\n",
        "    encoded_dialog = np.zeros((LN, LM), dtype=float)\n",
        "    for i in range(0, LN):\n",
        "        if i < len(s):\n",
        "            encoded_dialog[i] = to_categorical(message2idx[s[i]], num_classes=LM)\n",
        "        else:\n",
        "            encoded_dialog[i] = to_categorical(message2idx['<PAD>'], num_classes=LM)\n",
        "\n",
        "    encoded_dialogs.append(encoded_dialog)\n",
        "\n",
        "# print(encoded_dialogs)"
      ],
      "metadata": {
        "id": "IpQ2YdUKdOfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(encoded_dialogs)  # shape = (num_dialogs, LN, LM)\n",
        "\n",
        "dialogs_str = [' '.join(s) for s in seqs]\n",
        "labels, uniques = pd.factorize(dialogs_str)\n",
        "y_int = labels                    # integers 0..N-1\n",
        "y = to_categorical(y_int, num_classes=N)  # one-hot\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "EejppHLsPhBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 4) Training\n",
        "# -------------------\n",
        "model1 = build_model_1()\n",
        "history1 = model1.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=max_epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "model2 = build_model_2()\n",
        "history2 = model2.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=max_epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "kbkNmvzndRdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# IV.B Detection Performance\n",
        "# -------------------\n",
        "def detection_perf(model, X, y_true):\n",
        "    y_pred = model.predict(X, batch_size=batch_size)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    y_true_labels = np.argmax(y_true, axis=1)\n",
        "    acc = accuracy_score(y_true_labels, y_pred_labels)\n",
        "    return acc\n",
        "\n",
        "pd_train_1 = detection_perf(model1, X_train, y_train)\n",
        "pd_test_1  = detection_perf(model1, X_test,  y_test)\n",
        "print(f\"IV.B – Model1 Detection PD_train={pd_train_1:.4f}, PD_test={pd_test_1:.4f}\")\n",
        "\n",
        "pd_train_2 = detection_perf(model2, X_train, y_train)\n",
        "pd_test_2  = detection_perf(model2, X_test,  y_test)\n",
        "print(f\"IV.B – Model2 Detection PD_train={pd_train_2:.4f}, PD_test={pd_test_2:.4f}\")"
      ],
      "metadata": {
        "id": "8wIxLJ5DdUd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# IV.C Prediction Performance\n",
        "# -------------------\n",
        "def prediction_perf(model, X_pref, y_pref):\n",
        "    y_pred = model.predict(X_pref, batch_size=batch_size)\n",
        "    correct = (np.argmax(y_pred,1) == np.argmax(y_pref,1)).sum()\n",
        "    total   = len(y_pref)\n",
        "    return correct / total\n",
        "\n",
        "pe_train_1 = prediction_perf(model1, X_train_prefixes, y_train_prefixes)\n",
        "pe_test_1  = prediction_perf(model1, X_test_prefixes,  y_test_prefixes)\n",
        "print(f\"IV.C – Model1 Prediction PE_train={pe_train_1:.4f}, PE_test={pe_test_1:.4f}\")\n",
        "\n",
        "pe_train_2 = prediction_perf(model2, X_train_prefixes, y_train_prefixes)\n",
        "pe_test_2  = prediction_perf(model2, X_test_prefixes,  y_test_prefixes)\n",
        "print(f\"IV.C – Model2 Prediction PE_train={pe_train_2:.4f}, PE_test={pe_test_2:.4f}\")"
      ],
      "metadata": {
        "id": "tTFmhrq8dXjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# IV.D Detection of Unknown SIP Dialogs\n",
        "# -------------------\n",
        "# 1) Calculate threshold λM = mean(max_i yhat_i) on train_full dialogs\n",
        "yhat_train_full = model1.predict(X_train_full, batch_size=batch_size)\n",
        "max_train       = np.max(yhat_train_full, axis=1)\n",
        "lambda_M        = max_train.mean()\n",
        "\n",
        "# 2) Calculate threshold λS, λK on skewness and kurtosis of known dialogs\n",
        "sk_train       = skew(yhat_train_full, axis=1)\n",
        "ku_train       = kurtosis(yhat_train_full, axis=1)\n",
        "mu_S, var_S    = sk_train.mean(),  sk_train.var()\n",
        "mu_K, var_K    = ku_train.mean(),  ku_train.var()\n",
        "lambda_S       = mu_S - var_S\n",
        "lambda_K       = mu_K - var_K\n",
        "\n",
        "# 3) Classification functions\n",
        "def classify_max_threshold(yhat):\n",
        "    return np.where(np.max(yhat,axis=1) < lambda_M, -1, 0)  # -1 = unknown, 0 = known\n",
        "\n",
        "def classify_moments(yhat):\n",
        "    ske = skew(yhat,axis=1)\n",
        "    kur = kurtosis(yhat,axis=1)\n",
        "    return np.where((ske<lambda_S)&(kur<lambda_K), -1, 0)\n",
        "\n",
        "# 4) Prediction on unknown set (anomalous + test unknown)\n",
        "X_u = X_unknown_full\n",
        "y_true = np.full(len(X_u), -1)           # ground‐truth = unknown\n",
        "yhat_u = model1.predict(X_u, batch_size=batch_size)\n",
        "\n",
        "# 5) Build X_train_full\n",
        "X_k = X_train_full\n",
        "y_true_k = np.zeros(len(X_k), dtype=int)  # ground‐truth = known\n",
        "\n",
        "# 6) Evaluate\n",
        "X_all = np.vstack([X_k, X_u])\n",
        "y_true_all = np.concatenate([y_true_k, y_true])\n",
        "\n",
        "yhat_all = model1.predict(X_all, batch_size=batch_size)\n",
        "\n",
        "# 7) Classification\n",
        "y_pred_max = classify_max_threshold(yhat_all)\n",
        "y_pred_moments = classify_moments(yhat_all)\n",
        "\n",
        "# 8) Confusion matrix and metrics\n",
        "def report(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true==0, y_pred==0)\n",
        "    acc = accuracy_score(y_true,    y_pred)\n",
        "    prec = precision_score(y_true==0, y_pred==0)\n",
        "    rec  = recall_score(y_true==0,    y_pred==0)\n",
        "    f1   = f1_score(y_true==0,       y_pred==0)\n",
        "    return cm, acc, prec, rec, f1\n",
        "\n",
        "cm1, acc1, prec1, rec1, f11 = report(y_true_all, y_pred_max)\n",
        "cm2, acc2, prec2, rec2, f12 = report(y_true_all, y_pred_moments)\n",
        "\n",
        "print(\"IV.D – Max-Threshold Classifier\")\n",
        "print(\" Confusion Matrix:\\n\", cm1)\n",
        "print(f\" Accuracy={acc1:.4f}, Precision={prec1:.4f}, Recall={rec1:.4f}, F1={f11:.4f}\")\n",
        "\n",
        "print(\"IV.D – Skew/Kurtosis Classifier\")\n",
        "print(\" Confusion Matrix:\\n\", cm2)\n",
        "print(f\" Accuracy={acc2:.4f}, Precision={prec2:.4f}, Recall={rec2:.4f}, F1={f12:.4f}\")"
      ],
      "metadata": {
        "id": "RcOCVj1Tda57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}